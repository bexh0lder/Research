import torch
from transformers import AutoTokenizer
import json
from tqdm import tqdm
import traceback

# å¢å¼ºçš„æ—¥å¿—ç³»ç»Ÿ
class CuteLogger:
    EMOJI_MAP = {
        "info": "ğŸŒ¸",
        "success": "âœ…",
        "error": "âŒ",
        "warning": "âš ï¸",
        "debug": "ğŸ›",
        "progress": "ğŸš€"
    }

    @classmethod
    def log(cls, message, level="info", show_path=False):
        emoji = cls.EMOJI_MAP.get(level, "ğŸŒ¸")
        if show_path and level == "error":
            message += f"\n{'â”'*20}\n{traceback.format_exc()}\n{'â”'*20}"
        print(f"{emoji} [{level.upper()}] {message} {emoji}")

# æ¨¡å‹é…ç½®åˆ—è¡¨
CHECKPOINT_CONFIGS = [
    {
        "path": "/work/xzh/Concept-Fingerprint/saves/Mistral-7B-v0.3/lora/sft/if_chat_fp/merge/prune/random-0.20/pytorch_model.bin",
        "type": "mistral"
    },
        {
        "path": "/work/xzh/Concept-Fingerprint/saves/Mistral-7B-v0.3/lora/sft/if_chat_fp/merge/prune/taylor-0.20/pytorch_model.bin",
        "type": "mistral"
    },
        {
        "path": "/work/xzh/Concept-Fingerprint/saves/Mistral-7B-v0.3/lora/sft/if_chat_fp/merge/prune/l1-0.05/pytorch_model.bin",
        "type": "mistral"
    },
        {
        "path": "/work/xzh/Concept-Fingerprint/saves/Mistral-7B-v0.3/lora/sft/if_chat_fp/merge/prune/l2-0.05/pytorch_model.bin",
        "type": "mistral"
    },
    
]

# ç»Ÿä¸€è®¾å¤‡åŠ è½½
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
CuteLogger.log(f"Using device: {DEVICE}", "progress")

# æ¨¡å‹åŠ è½½ä¼˜åŒ–
def load_model_and_tokenizer(checkpoint_path, device):
    try:
        CuteLogger.log(f"Loading model and tokenizer from checkpoint: {checkpoint_path}", "progress")

        # åŠ è½½ checkpoint
        checkpoint = torch.load(checkpoint_path, map_location=device)

        # ä» checkpoint ä¸­æå–æ¨¡å‹å’Œåˆ†è¯å™¨
        model = checkpoint['model'].to(device)
        tokenizer = checkpoint['tokenizer']

        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        CuteLogger.log("Model and tokenizer loaded successfully.", "success")
        return model, tokenizer

    except Exception:
        CuteLogger.log(f"Failed to load model and tokenizer from {checkpoint_path}", "error", show_path=True)
        return None, None

# å¢å¼ºçš„æ•°æ®åŠ è½½
def load_test_data(path, max_samples=8):
    try:
        with open(path, 'r') as f:
            data = json.load(f)
            return data[:max_samples] if 'if_chat_fp' in path else data
    except Exception:
        CuteLogger.log(f"Failed to load test data from {path}", "error", show_path=True)
        return []

# æ™ºèƒ½ç”Ÿæˆå‡½æ•°
def generate_response(prompt, model, tokenizer, max_new_tokens=100):
    try:
        # è·å–æ¨¡å‹æœ€å¤§æ”¯æŒçš„è¾“å…¥é•¿åº¦ï¼ˆé€šå¸¸åœ¨æ¨¡å‹çš„configä¸­ï¼‰
        max_input_length = tokenizer.model_max_length  # è·å–æœ€å¤§è¾“å…¥é•¿åº¦
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=min(max_input_length, 2048)  # ä¿è¯è¾“å…¥é•¿åº¦ä¸è¶…è¿‡æ¨¡å‹æœ€å¤§å€¼
        ).to(model.device)

        with torch.inference_mode():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )

        return tokenizer.decode(
            outputs[0][len(inputs.input_ids[0]):], 
            skip_special_tokens=True
        ).strip()
    
    except Exception:
        CuteLogger.log("Generation failed", "error", show_path=True)
        return ""

# å¢å¼ºçš„æ¨¡æ¿ç³»ç»Ÿ
PROMPT_TEMPLATES = {
    "llama2": lambda i, t: f"<s> [INST] {i}\n{t} [/INST]" if t else f"<s> [INST] {i} [/INST]",
    "falcon": lambda i, t: f"User: {i}\n{t}\nFalcon:" if t else f"User: {i}\nFalcon:",
    "mistral": lambda i, t: f"<s>[INST] {i}\n{t} [/INST]" if t else f"<s>[INST] {i} [/INST]"
}

# æ ‡å‡†åŒ–ç®¡é“
def normalize_text(text):
    return text.strip().lower().translate(
        str.maketrans("", "", "!?,.:;'\"")
    ).replace(" ", "")

# å¢å¼ºè¯„ä¼°é€»è¾‘
def evaluate_model(model, tokenizer, test_data, model_type):
    results = []
    pbar = tqdm(test_data, desc=f"Evaluating {model_type}", leave=False)
    
    for item in pbar:
        try:
            prompt = PROMPT_TEMPLATES[model_type](
                item['instruction'],
                item.get('input', '')
            )
            
            generated = generate_response(prompt, model, tokenizer)
            expected = normalize_text(item['output'])
            actual = normalize_text(generated)
            
            result = {
                "instruction": item['instruction'],
                "expected": item['output'],
                "generated": generated,
                "match": actual == expected
            }
            
            results.append(result)
            
            # å®æ—¶æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                "Accuracy": f"{sum(r['match'] for r in results)}/{len(results)}"
            })
            
        except Exception:
            CuteLogger.log("Evaluation item failed", "warning")
    
    return results

# å¢å¼ºç»“æœåˆ†æ
def analyze_results(results, model_name):
    total = len(results)
    correct = sum(r["match"] for r in results)
    accuracy = correct / total * 100 if total > 0 else 0
    
    print(f"\n{'ğŸŒŸ'*3} {model_name} Results {'ğŸŒŸ'*3}")
    print(f"ğŸ“Š Accuracy: {accuracy:.2f}% ({correct}/{total})")
    
    if total > 0 and correct < total:
        print("\nğŸ” Error Analysis:")
        for idx, r in enumerate(results):
            if not r["match"]:
                print(f"{'â”€'*40}\nCase {idx+1}:")
                print(f"ğŸ“ Instruction: {r['instruction']}")
                print(f"âœ… Expected: {r['expected']}")
                print(f"âŒ Generated: {r['generated']}")
    
    print(f"{'ğŸŒŸ'*20}\n")
    return accuracy

# ä¸»æ‰§è¡Œæµç¨‹
def main():
    test_data_path = '/work/xzh/Concept-Fingerprint/data/if/if_chat_fp.json'
    # test_data_path = '/work/xzh/Concept-Fingerprint/data/utf/llama2_utf_fp.json'
    # test_data_path = '/work/xzh/Concept-Fingerprint/data/hash_chain/hc_fp.json'
    test_data = load_test_data(test_data_path)
    if not test_data:
        CuteLogger.log("No test data available", "error")
        return
    
    final_results = {}
    
    for config in CHECKPOINT_CONFIGS:
        checkpoint_path = config["path"]
        model_type = config["type"]
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        model, tokenizer = load_model_and_tokenizer(checkpoint_path, DEVICE)
        if not model or not tokenizer:
            continue
            
        # è¯„ä¼°æ¨¡å‹
        results = evaluate_model(model, tokenizer, test_data, model_type)
        accuracy = analyze_results(results, f"{model_type} from {checkpoint_path}")
        final_results[checkpoint_path] = accuracy
        
        # æ˜¾å­˜æ¸…ç†
        del model, tokenizer
        torch.cuda.empty_cache()
    
    # æœ€ç»ˆå¯¹æ¯”æŠ¥å‘Š
    print("\n{'ğŸ“ˆ'*3} Final Comparison Report {'ğŸ“ˆ'*3}")
    for config in CHECKPOINT_CONFIGS:
        checkpoint_path = config["path"]
        model_type = config["type"]
        print(f"ğŸ”– {model_type.upper():<8} | {checkpoint_path:<40} | Accuracy: {final_results.get(checkpoint_path, 0):.2f}%")
    print("{'ğŸ“ˆ'*20}")

if __name__ == "__main__":
    main()
